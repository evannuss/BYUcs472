{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_5_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# Unsupervised Learning: Clustering Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (50%) Implement the k-means clustering algorithm and the HAC (Hierarchical Agglomerative Clustering) algorithm.\n",
        "\n",
        "### 1.1.1 HAC\n",
        "\n",
        "### Code requirements \n",
        "- HAC should support both single link and complete link options.\n",
        "- HAC automatically generates all clusterings from n to 1.  To simplify the amount of output you may want to implement a mechanism to specify for which k values actual output will be generated.\n",
        "\n",
        "\n",
        "---\n",
        "The output should include the following:\n",
        "- The number of clusters (k).\n",
        "- The total SSE of the full clustering. \n",
        "\n",
        "\n",
        "For each cluster report include:\n",
        "\n",
        "\n",
        "- The centroid id.\n",
        "- The number of instances tied to that centroid. \n",
        "* The SSE of that cluster. (The sum squared error (SSE) of a single cluster is the sum of the squared euclidean distance of each cluster member to the cluster centroid.)\n",
        "---\n",
        "You only need to handle continuous features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "source": [
        "class HACClustering(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self,k=3,link_type='single'): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            link_type = single or complete. when combining two clusters use complete link or single link\n",
        "        \"\"\"\n",
        "        self.link_type = link_type\n",
        "        self.k = k\n",
        "\n",
        "    def get_distances(self, clusters):\n",
        "      distances = np.ones((len(clusters), len(clusters))) * np.inf\n",
        "      for i in range(len(clusters)):\n",
        "        for j in range(i+1, len(clusters)):\n",
        "          # d = np.linalg.norm(clusters[i] - clusters[j], ord=2, axis=1)\n",
        "          d = distance.cdist(clusters[i], clusters[j])\n",
        "\n",
        "          if self.link_type == 'single':\n",
        "            distances[i][j] = np.argmin(d)\n",
        "          elif self.link_type == 'complete':\n",
        "            d = np.where(d==np.inf, 0, d)\n",
        "            distances[i][j] = np.argmax(d)\n",
        "\n",
        "      return distances\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "\n",
        "        num_clusters = X.shape[0]\n",
        "        centroids = X[:self.k]\n",
        "        clusters = []\n",
        "        for x in X:\n",
        "          x.shape = (1,X.shape[1])\n",
        "          clusters.append(x)\n",
        "\n",
        "        # print(len(clusters))\n",
        "        while num_clusters > self.k:\n",
        "          curr_distances = self.get_distances(clusters)\n",
        "\n",
        "          ind = np.unravel_index(np.argmin(curr_distances, axis=None), curr_distances.shape)\n",
        "\n",
        "          new_cluster = np.concatenate((clusters[ind[0]], clusters[ind[1]]), axis=0)\n",
        "          del clusters[ind[1]]\n",
        "          del clusters[ind[0]]\n",
        "          clusters.append(new_cluster)\n",
        "          \n",
        "          num_clusters -= 1\n",
        "        \n",
        "        self.clusters = clusters\n",
        "\n",
        "        return self\n",
        "\n",
        "    def sse(self, centroid, cluster):\n",
        "      distance = np.array([np.linalg.norm(cluster - centroid, ord=2, axis=1)])\n",
        "      distance_squared = np.square(distance)\n",
        "      return np.sum(distance_squared)\n",
        "\n",
        "    def print_clusters(self):\n",
        "        \"\"\"\n",
        "            Used for grading.\n",
        "            print(\"{:d}\\n\".format(k))\n",
        "            print(\"{:.4f}\\n\\n\".format(total SSE))\n",
        "            for each cluster and centroid:\n",
        "                print(np.array2string(centroid,precision=4,separator=\",\"))\n",
        "                print(\"\\n\")\n",
        "                print(\"{:d}\\n\".format(size of cluster))\n",
        "                print(\"{:.4f}\\n\\n\".format(SSE of cluster))\n",
        "        \"\"\"\n",
        "        all_sse = []\n",
        "        centroids = []\n",
        "\n",
        "        for c in self.clusters:\n",
        "          centroid = c.mean(0)\n",
        "          centroids.append(centroid)\n",
        "\n",
        "          all_sse.append(self.sse(centroid, c))\n",
        "\n",
        "        all_sse = np.array(all_sse)\n",
        "        print(\"{:d}\\n\".format(self.k))\n",
        "        print(\"{:.4f}\\n\\n\".format(np.sum(all_sse)))\n",
        "\n",
        "        for i in range(len(self.clusters)):\n",
        "          print(np.array2string(centroids[i],precision=4,separator=\",\"))\n",
        "          print(\"{:d}\".format(self.clusters[i].shape[0]))\n",
        "          print(\"{:.4f}\\n\\n\".format(all_sse[i]))\n",
        "      "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZMYmE4Mepci"
      },
      "source": [
        "def normalize(X):\n",
        "  for i in range(X.shape[1]):\n",
        "    X[:,i] = (X[:,i] - np.min(X[:,i])) / (np.max(X[:,i]) - np.min(X[:,i]))\n",
        "  return X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "### 1.1.2 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "\n",
        "\n",
        "---\n",
        "The dataset was modified to be a lot smaller. The last datapoint should be on line 359 or the point 0.585,0.46,0.185,0.922,0.3635,0.213,0.285,10. The remaining points should be commented out.\n",
        "\n",
        "\n",
        "\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use the first k instances as the initial centroids\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting SSE and centroid values.\n",
        "\n",
        "\n",
        "---\n",
        "Solutions in files:\n",
        "\n",
        "[Debug HAC Single.txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20HAC%20Single%20Link.txt)\n",
        "\n",
        "[Debug HAC Complete.txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20HAC%20Complete%20Link.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTm0hfORGVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d416d0a1-ea35-43ac-a6ec-8af493168e8d"
      },
      "source": [
        "## DEBUG SET ##\n",
        "\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "# Download file with curl\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff --output debug-training-dataset.arff\n",
        "\n",
        "data = arff.loadarff('debug-training-dataset.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "\n",
        "data = np.array(df)\n",
        "X = normalize(data)\n",
        "# print(X.shape)\n",
        "\n",
        "single_debug_HAC = HACClustering(5, 'single')\n",
        "single_debug_HAC.fit(X)\n",
        "print(\"SINGLE LINK*************************************\")\n",
        "single_debug_HAC.print_clusters()\n",
        "\n",
        "complete_debug_HAC = HACClustering(5, 'complete')\n",
        "complete_debug_HAC.fit(X)\n",
        "print(\"\\n\\n\\nCOMPLETE LINK********************************\")\n",
        "complete_debug_HAC.print_clusters()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  187k  100  187k    0     0  2087k      0 --:--:-- --:--:-- --:--:-- 2087k\n",
            "SINGLE LINK*************************************\n",
            "5\n",
            "\n",
            "58.9879\n",
            "\n",
            "\n",
            "[0.6526,0.6438,0.5186,0.335 ,0.319 ,0.3506,0.2766,0.417 ]\n",
            "34\n",
            "9.9565\n",
            "\n",
            "\n",
            "[0.7245,0.7224,0.614 ,0.388 ,0.3687,0.4355,0.301 ,0.4412]\n",
            "24\n",
            "2.8964\n",
            "\n",
            "\n",
            "[0.5991,0.5961,0.5019,0.2841,0.2706,0.2939,0.2335,0.3824]\n",
            "82\n",
            "24.9125\n",
            "\n",
            "\n",
            "[0.5489,0.538 ,0.4382,0.2533,0.2344,0.2403,0.2173,0.3853]\n",
            "60\n",
            "21.2225\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "COMPLETE LINK********************************\n",
            "5\n",
            "\n",
            "38.7344\n",
            "\n",
            "\n",
            "[0.5093,0.4979,0.3961,0.1989,0.1973,0.2155,0.1589,0.2931]\n",
            "60\n",
            "12.6495\n",
            "\n",
            "\n",
            "[0.5942,0.5883,0.4822,0.2586,0.2481,0.2681,0.2098,0.3843]\n",
            "62\n",
            "10.3419\n",
            "\n",
            "\n",
            "[0.5238,0.5106,0.4241,0.2165,0.2159,0.2254,0.1689,0.3443]\n",
            "34\n",
            "8.1156\n",
            "\n",
            "\n",
            "[0.828 ,0.8335,0.7213,0.5424,0.4861,0.5381,0.4667,0.5936]\n",
            "44\n",
            "7.6273\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3VNB1ui03N"
      },
      "source": [
        "### 1.1.3 Evaluation\n",
        "\n",
        "We will evaluate your model based on its print_clusters() output using [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAxA78QjDh2"
      },
      "source": [
        "# Load evaluation data\n",
        "\n",
        "# Train on evaluation data\n",
        "\n",
        "# Print clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT8Ckn7RRGVz"
      },
      "source": [
        "### 1.2.1 K-Means\n",
        "\n",
        "### Code requirements \n",
        "- Ability to choose k and specify k initial centroids\n",
        "- Use Euclidean Distance as metric\n",
        "- Ability to handle distance ties\n",
        "- Include output label as a cluster feature\n",
        "\n",
        "\n",
        "---\n",
        "The output should include the following:\n",
        "- The number of clusters (k).\n",
        "- The total SSE of the full clustering. \n",
        "\n",
        "\n",
        "For each cluster report include:\n",
        "\n",
        "\n",
        "- The centroid id.\n",
        "- The number of instances tied to that centroid. \n",
        "- The SSE of that cluster. (The sum squared error (SSE) of a single cluster is the sum of the squared euclidean distance of each cluster member to the cluster centroid.)\n",
        "---\n",
        "You only need to handle continuous features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGok0oESRGV0"
      },
      "source": [
        "class KMEANSClustering(BaseEstimator,ClusterMixin):\n",
        "\n",
        "    def __init__(self,k=3,debug=False): ## add parameters here\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            k = how many final clusters to have\n",
        "            debug = if debug is true use the first k instances as the initial centroids otherwise choose random points as the initial centroids.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.debug = debug\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data\n",
        "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        return self\n",
        "    \n",
        "    def print_clusters(self):\n",
        "        \"\"\"\n",
        "            Used for grading.\n",
        "            print(\"{:d}\\n\".format(k))\n",
        "            print(\"{:.4f}\\n\\n\".format(total SSE))\n",
        "            for each cluster and centroid:\n",
        "                print(np.array2string(centroid,precision=4,separator=\",\"))\n",
        "                print(\"\\n\")\n",
        "                print(\"{:d}\\n\".format(size of cluster))\n",
        "                print(\"{:.4f}\\n\\n\".format(SSE of cluster))\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIQDnaXdRGV0"
      },
      "source": [
        "### 1.2.2 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
        "\n",
        "\n",
        "- Train until convergence\n",
        "- Normalize Data\n",
        "- K = 5\n",
        "- Use the first k instances as the initial centroids\n",
        "- Use 4 decimal places and DO NOT ROUND when reporting SSE and centroid values\n",
        "\n",
        "\n",
        "---\n",
        "Solutions in files:\n",
        "\n",
        "[Debug K Means.txt](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/Debug%20K%20Means.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgAyy82gixIF"
      },
      "source": [
        "# Load debug data\n",
        "\n",
        "# Train on debug data\n",
        "\n",
        "# Print clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywlVL4rkRGV1"
      },
      "source": [
        "### 1.2.3 Evaluation\n",
        "\n",
        "We will evaluate your model based on its print_clusters() output using [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/seismic-bumps_train.arff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0kZR-1rRGV1"
      },
      "source": [
        "# Load evaluation data\n",
        "\n",
        "# Train on evaluation data\n",
        "\n",
        "# Print clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2.1.1 (7.5%) Clustering the Iris Classification problem - HAC\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "\n",
        "- Use single-link and complete link clustering algorithms\n",
        "- State whether you normalize your data or not (your choice).  \n",
        "- Show your results for clusterings using k = 2-7.  \n",
        "- Graph the total SSE for each k and discuss your results (i.e. what kind of clusters are being made).\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SSoasDQSKXb"
      },
      "source": [
        "# Iris Classification using single-link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xa_8jwPRGV3"
      },
      "source": [
        "# Iris Classification using complete-link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NWIjuKmRGV3"
      },
      "source": [
        "Discuss differences between single-link and complete-link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wes60ypiRGV3"
      },
      "source": [
        "## 2.1.2 (5%) Clustering the Iris Classification problem - HAC\n",
        "\n",
        "Requirements:\n",
        "- Repeat excercise 2.1.1 and include the output label as one of the input features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3ztmhT3RGV4"
      },
      "source": [
        "# Clustering Labels using single-link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__6aHtgcRGV4"
      },
      "source": [
        "# Clustering Labels using complete-link"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAe_NCp_RGV5"
      },
      "source": [
        "Discuss any differences between the results from 2.1.1 and 2.1.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2dpMq2NRGV5"
      },
      "source": [
        "## 2.2.1 (7.5%) Clustering the Iris Classification problem: K-Means\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "\n",
        "Run K-Means on the Iris dataset using the output label as a feature and without using the output label as a feature\n",
        "\n",
        "Requirements:\n",
        "- State whether you normalize your data or not (your choice).  \n",
        "- Show your results for clusterings using k = 2-7.  \n",
        "- Graph the total SSE for each k and discuss your results (i.e. what kind of clusters are being made).\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFLSwndYRGV5"
      },
      "source": [
        "# Iris Classification without output label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3iuPLmuRGV6"
      },
      "source": [
        "# Iris Classification with output label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXRTNCgRGV6"
      },
      "source": [
        "Compare results and differences between using the output label and excluding the output label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7foinMVuRGV6"
      },
      "source": [
        "## 2.2.2 (5%) Clustering the Iris Classification problem: K-Means\n",
        "\n",
        "Requirements:\n",
        "- Use the output label as an input feature\n",
        "- Run K-Means 5 times with k=4, each time with different initial random centroids and discuss any variations in the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNvV4LMyRGV7"
      },
      "source": [
        "#K-Means 5 times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhLxcvs3RGV7"
      },
      "source": [
        "Discuss any variations in the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 3.1 (12.5%) Run the SK versions of HAC (both single and complete link) on iris including the output label and compare your results with those above.\n",
        "Use the silhouette score for this iris problem(k = 2-7).  You may write your own code to do silhouette (optional extra credit) or you can use sklearn.metrics.silhouette_score. Please state if you coded your own silhouette score function to receive the extra credit points (described below). Discuss how helpful Silhouette appeared to be for selecting which clustering is best. You do not need to supply full Silhouette graphs, but you could if you wanted to.\n",
        "\n",
        "Requirements\n",
        "- Use the Sillhouette score for this iris problem (k= 2-7) \n",
        "- Use at least one other scoring function from [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) and compare the results. State which metric was used. \n",
        "- Possible sklean metrics include (* metrics require ground truth labels):\n",
        "    - adjusted_mutual_info_score*\n",
        "    - adjusted_rand_score*\n",
        "    - homogeneity_score*\n",
        "    - completeness_score*\n",
        "    - fowlkes_mallows_score*\n",
        "    - calinski_harabasz_score\n",
        "    - davies_bouldin_score\n",
        "- Experiment using different hyper-parameters. Discuss Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFQv70W2VyqJ"
      },
      "source": [
        "# Load sklearn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Record impressions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIGOCJw2RGV8"
      },
      "source": [
        "## 3.2 (12.5%) Run the SK version of k-means on iris including the output label and compare your results with those above. \n",
        "\n",
        "Use the silhouette score for this iris problem(k = 2-7). You may write your own code to do silhouette (optional extra credit) or you can use sklearn.metrics.silhouette_score. Please state if you coded your own silhouette score function to receive the extra credit points (described below). Discuss how helpful Silhouette appeared to be for selecting which clustering is best. You do not need to supply full Silhouette graphs, but you could if you wanted to.\n",
        "\n",
        "Requirements\n",
        "- Use the Sillhouette score for this iris problem (k= 2-7) \n",
        "- Use at least one other scoring function form sklearn.metrics and compare the results. State which metric was used\n",
        "- Experiment different hyper-parameters. Discuss Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx_knrmPRGV8"
      },
      "source": [
        "# Load sklearn \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9-Wz_hcRGV9"
      },
      "source": [
        "*Record impressions*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 4. (Optional 5% extra credit) For your silhouette experiment above, write and use your own code to calculate the silhouette scores, rather than the SK or other version. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u90XkMkRGV9"
      },
      "source": [
        "*Show findings here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1JVfnXVRGV-"
      },
      "source": [
        "# Copy function Below"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}